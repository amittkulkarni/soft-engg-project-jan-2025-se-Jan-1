# -*- coding: utf-8 -*-
"""Academic RAG  Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bq0zGy5ijoQxFIrOonJT0dQ_GQGAyua-

**All the pip installs**
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet  langchain-google-genai
# %pip install --upgrade --quiet langchain-community langchainhub langchain-chroma pypdf
!pip install weasyprint

"""**Importing all the nexessary Libraries**"""

import re
import os
import warnings
warnings.filterwarnings('ignore')
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain import hub
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import create_history_aware_retriever
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from IPython.display import display
from IPython.display import Markdown
import textwrap
import logging
import sqlite3
from datetime import datetime

"""**The Main Chatbot Code**"""

os.environ["GOOGLE_API_KEY"] = "Google_Api_key"

gemini_embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", convert_system_message_to_human=True)

loader = PyPDFLoader("MLP Week 4 Slides.pdf")
doc = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
splits = text_splitter.split_documents(doc)

vector_store = Chroma.from_documents(documents=splits, embedding=gemini_embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})


system_prompt = (
    "You are an AI assistant specialized in Machine Learning Practice (MLP). "
    "You should provide detailed, structured answers with bullet points and examples. "
    "If the question is about general conversation, respond accordingly as a chatbot. "
    "Otherwise, use retrieved MLP-related documents to answer."
    "\n\n"
    "{context}"
)

retriever_prompt = (
    "You have to remember the chat history of users."
    "You should remember the user prompts and the answers you give."
    "Make sense of this chat history and answer accordingly when user references context in the chat history."
)

chat_prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
contextualize_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", retriever_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)

qa_prompt = ChatPromptTemplate.from_messages(
    [("system", system_prompt), MessagesPlaceholder("chat_history"), ("human", "{input}")]
)

qachain = create_stuff_documents_chain(model, chat_prompt)
chat_history_retriever = create_history_aware_retriever(model, retriever, contextualize_prompt)
answer_chain = create_stuff_documents_chain(model, qa_prompt)

RAG_chain = create_retrieval_chain(chat_history_retriever, answer_chain)

# Stores Chat History for each session
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """Retrieve or create chat history for a given session."""
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

conversation_RAG_Chain = RunnableWithMessageHistory(
    RAG_chain,
    get_session_history,
    input_messages_keys="input",
    history_messages_key="chat_history",
    output_message_keys="answer",
)

# Function to Handle Chat with Memory

def chat(input_text, session_id="chat1"):
    """Handles chat with persistent memory"""

    # Retrieve chat history for this session
    chat_history = get_session_history(session_id)

    # Invoke the RAG chain
    response_data = conversation_RAG_Chain.invoke(
        {"input": input_text, "chat_history": chat_history.messages},
        config={"configurable": {"session_id": session_id}}
    )


    # Check if the expected key 'answer' exists in the response and handle missing key gracefully
    response = response_data.get("answer", "Sorry, I could not retrieve an answer. Please try again.")

    # Append messages to history
    chat_history.add_user_message(input_text)
    chat_history.add_ai_message(response)

    return response

import logging

# Configure logging to ignore specific warnings
logging.getLogger('langchain_core.callbacks.manager').setLevel(logging.ERROR)


def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

to_markdown(chat("You have the MLP week 4 data, right?", session_id="chat2"))