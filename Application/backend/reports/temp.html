<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>$4.1 Linear regression</title>
<style>
/* CSS Reset for cross-platform consistency */
            * {
            margin: 0;
padding: 0;
box-sizing: border-box;
-webkit-box-sizing: border-box;
            }
/* Base styles with WebKit prefixes */
            body {
            font-family: Arial, Helvetica, sans-serif;
line-height: 1.6;
color: #333333;
padding: 20px;
background-color: #ffffff;
            }

/* Header styles with gradient fallback */
            h1 {
            font-size: 24pt;
color: #2c3e50;
margin-top: 0.5em;
margin-bottom: 0.5em;
padding-bottom: 10px;
border-bottom: 3px solid #3498db;
            }

            h2 {
            font-size: 18pt;
color: #2c3e50;
margin-top: 1.2em;
margin-bottom: 0.6em;
border-bottom: 1px solid #dfe6e9;
padding-bottom: 5px;
            }

            h3 {
            font-size: 14pt;
color: #2c3e50;
margin-top: 1em;
margin-bottom: 0.5em;
            }

            p {
            margin-bottom: 1em;
            }

/* Old-school layouts instead of flexbox */
            .row {
            display: block;
width: 100%;
overflow: hidden; /* clearfix */
            }

            .col {
            float: left;
width: 33%; /* For 3 columns - adjust as needed */
padding: 10px;
            }

/* Code blocks with simpler styling */
            pre {
            background-color: #f5f7fa;
border: 1px solid #dfe6e9;
border-radius: 4px;
padding: 10px;
margin: 15px 0;
overflow-x: auto;
            }

            code {
            font-family: Consolas, Monaco, monospace;
font-size: 0.9em;
color: #1E847F;
            }

/* Tables with simple styling */
            table {
            border-collapse: collapse;
width: 100%;
margin: 15px 0;
            }

            th {
            background-color: #3498db;
color: #ffffff;
font-weight: bold;
text-align: left;
padding: 8px;
border: 1px solid #dfe6e9;
            }

            td {
            padding: 8px;
border: 1px solid #dfe6e9;
            }

            tr:nth-child(even) {
            background-color: #f5f7fa;
            }

/* Lists with simple styling */
            ul, ol {
            padding-left: 20px;
margin: 10px 0;
            }

            li {
            margin-bottom: 5px;
            }

/* Math rendering with simpler styling */
            .math-display {
            display: block;
padding: 10px;
margin: 15px 0;
text-align: center;
font-family: 'Times New Roman', serif;
background-color: #f5f7fa;
border-left: 3px solid #3498db;
            }

            .math-inline {
            font-family: 'Times New Roman', serif;
padding: 0 3px;
background-color: #f5f7fa;
            }

/* Print styles */
            @media print{
            body {
                padding: 0;
                }

                @page {
                    margin: 2cm;
                }
            }
            </style>
            </head>
            <body>
            <div class="content">
    <h2>Training and Predicting with Linear Regression Models in scikit-learn</h2>
<p>In this lecture, Professor Ashish explains how to train and make predictions using linear regression models in scikit-learn, covering both the normal equation method and iterative optimization using stochastic gradient descent (SGD).</p>
<h3>Training Linear Regression Models</h3>
<p>The lecture details two approaches for training linear regression models:</p>
<ol>
<li>
<p><strong>Normal Equation:</strong> This method uses the <code>LinearRegression</code> class from <code>sklearn.linear_model</code>.  The training process involves instantiating the <code>LinearRegression</code> object and then calling the <code>fit</code> method with the training feature matrix (<code>X_train</code>) and label vector (<code>y_train</code>).</p>
</li>
<li>
<p><strong>Iterative Optimization (SGD):</strong> This approach employs the <code>SGDRegressor</code> class, suitable for larger datasets.  The <code>fit</code> method is used similarly, but <code>SGDRegressor</code> offers more control over the optimization process through various parameters.</p>
</li>
</ol>
<h3><code>SGDRegressor</code> Parameters and Optimization</h3>
<p>The lecture emphasizes the importance of several <code>SGDRegressor</code> parameters:</p>
<ul>
<li><strong><code>learning_rate</code>:</strong>  Controls the step size during optimization.  Inverse scaling is often used, calculated as ( \eta_t = \frac{\eta_0}{t^{power_t}} ), where (\eta_0) is the initial learning rate, (t) is the current iteration, and <code>power_t</code> is a hyperparameter.</li>
<li><strong><code>average</code>:</strong> Enables averaging of the weights after a specified number of samples, improving performance with many features and a high (\eta_0).  Setting <code>average</code> to an integer (n) starts averaging after seeing (n) samples.</li>
<li><strong><code>warm_start = True</code>:</strong> This allows initializing SGD with the weight vector from a previous run, useful for monitoring loss iteration by iteration.  Setting <code>max_iter = 1</code> and iteratively calling <code>fit</code> with <code>warm_start = True</code> enables this.</li>
</ul>
<blockquote>
<p><strong>Tip:</strong> Always set a <code>random_state</code> in the <code>SGDRegressor</code> constructor for reproducible results.</p>
</blockquote>
<h3>Making Predictions</h3>
<p>Prediction on new data involves two steps:</p>
<ol>
<li>Arrange the data into a feature matrix (shape (#samples, #features)) or a sparse matrix.</li>
<li>Call the <code>predict</code> method on the trained linear regression object (either <code>LinearRegression</code> or <code>SGDRegressor</code>) with the feature matrix as an argument.  The method returns the predicted labels.</li>
</ol>
<h3>Feature Scaling and DummyRegressor</h3>
<p>The lecture also touches upon feature scaling, highlighting that SGD is sensitive to it.  It recommends using <code>StandardScaler</code> for preprocessing.  Finally, it briefly introduces <code>DummyRegressor</code>, a baseline model that predicts using strategies like mean, median, quantile, or a constant value.</p>
<p>Prof. Ashish concludes by summarizing the key steps in training and using linear regression models in scikit-learn, emphasizing the flexibility and control offered by <code>SGDRegressor</code> for large datasets and the importance of proper parameter tuning and feature scaling.</p>
    </div>
    </body>
    </html>
    